<!DOCTYPE html><html lang="en" class="scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/bio.jpg"/><link rel="stylesheet" href="/_next/static/css/4c59616ffa01abc8.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-af78f5c2c11af041.js"/><script src="/_next/static/chunks/4bd1b696-70b6d399998de86a.js" async=""></script><script src="/_next/static/chunks/684-5aaa8290a129f299.js" async=""></script><script src="/_next/static/chunks/main-app-8967dbb337c2822d.js" async=""></script><script src="/_next/static/chunks/17-9c04c9413a9f9f1f.js" async=""></script><script src="/_next/static/chunks/874-6cc630662f3664af.js" async=""></script><script src="/_next/static/chunks/862-15038af301665bb3.js" async=""></script><script src="/_next/static/chunks/app/layout-a118809a65dd8e19.js" async=""></script><script src="/_next/static/chunks/178-595a94b9af1e67b5.js" async=""></script><script src="/_next/static/chunks/748-b0753eeaf12cb586.js" async=""></script><script src="/_next/static/chunks/app/page-31460eb0e55869a4.js" async=""></script><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="dns-prefetch" href="https://google-fonts.jialeliu.com"/><link rel="preconnect" href="https://google-fonts.jialeliu.com" crossorigin=""/><link rel="preload" as="style" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/><link rel="stylesheet" id="gfonts-css" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap" media="print"/><script>
              (function(){
                var l = document.getElementById('gfonts-css');
                if (!l) return;
                if (l.media !== 'all') {
                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });
                }
              })();
            </script><noscript><link rel="stylesheet" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/></noscript><script>
              try {
                const theme = localStorage.getItem('theme-storage');
                const parsed = theme ? JSON.parse(theme) : null;
                const setting = parsed?.state?.theme || 'system';
                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));
                var root = document.documentElement;
                root.classList.add(effective);
                root.setAttribute('data-theme', effective);
              } catch (e) {
                var root = document.documentElement;
                root.classList.add('light');
                root.setAttribute('data-theme', 'light');
              }
            </script><title>Qiugang Zhan</title><meta name="description" content="Lecturer at the Southwestern University of Finance and Economics (SWUFE)."/><meta name="author" content="Qiugang Zhan"/><meta name="keywords" content="Qiugang Zhan,PhD,Research,Southwestern University of Finance and Economics (SWUFE)"/><meta name="creator" content="Qiugang Zhan"/><meta name="publisher" content="Qiugang Zhan"/><meta property="og:title" content="Qiugang Zhan"/><meta property="og:description" content="Lecturer at the Southwestern University of Finance and Economics (SWUFE)."/><meta property="og:site_name" content="Qiugang Zhan&#x27;s Academic Website"/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Qiugang Zhan"/><meta name="twitter:description" content="Lecturer at the Southwestern University of Finance and Economics (SWUFE)."/><link rel="icon" href="/favicon.svg"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans antialiased"><div style="visibility:hidden"><nav class="fixed top-0 left-0 right-0 z-50" data-headlessui-state=""><div class="transition-all duration-300 ease-out bg-transparent" style="transform:translateY(-100px)"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center h-16 lg:h-20"><div class="flex-shrink-0" tabindex="0"><a class="text-xl lg:text-2xl font-serif font-semibold text-primary hover:text-accent transition-colors duration-200" href="/">Qiugang Zhan</a></div><div class="hidden lg:block"><div class="ml-10 flex items-center space-x-8"><div class="flex items-baseline space-x-1"><a class="relative px-3 py-2 text-sm font-medium rounded-lg transition-colors duration-150 text-primary" href="/"><span class="relative z-10">About</span><div class="absolute inset-0 rounded-lg bg-accent/10"></div></a><a class="relative px-3 py-2 text-sm font-medium rounded-lg transition-colors duration-150 text-neutral-600" href="/publications/"><span class="relative z-10">Publications</span></a><a class="relative px-3 py-2 text-sm font-medium rounded-lg transition-colors duration-150 text-neutral-600" href="/teaching/"><span class="relative z-10">Teaching</span></a><a class="relative px-3 py-2 text-sm font-medium rounded-lg transition-colors duration-150 text-neutral-600" href="/services/"><span class="relative z-10">Services</span></a><a class="relative px-3 py-2 text-sm font-medium rounded-lg transition-colors duration-150 text-neutral-600" href="/cv/"><span class="relative z-10">CV</span></a></div><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div></div></div><div class="lg:hidden flex items-center space-x-2"><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div><button class="inline-flex items-center justify-center p-2 rounded-md text-neutral-600 hover:text-primary hover:bg-neutral-100 dark:hover:bg-neutral-800 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-accent transition-colors duration-200" id="headlessui-disclosure-button-Â«R5pdbÂ»" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open main menu</span><div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="block h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></div></button></div></div></div></div></nav><main class="min-h-screen pt-16 lg:pt-20"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen"><div class="grid grid-cols-1 lg:grid-cols-3 gap-12"><div class="lg:col-span-1"><div class="sticky top-8" style="opacity:0;transform:translateY(20px)"><div class="w-64 h-64 mx-auto mb-6 rounded-2xl overflow-hidden shadow-lg hover:shadow-xl transition-all duration-200 hover:scale-105"><img alt="Qiugang Zhan" width="256" height="256" decoding="async" data-nimg="1" class="w-full h-full object-cover object-[32%_center]" style="color:transparent" src="/bio.jpg"/></div><div class="text-center mb-6"><h1 class="text-3xl font-serif font-bold text-primary mb-2">Qiugang Zhan</h1><p class="text-lg text-accent font-medium mb-1">PhD</p><p class="text-neutral-600 mb-2">Southwestern University of Finance and Economics (SWUFE)</p></div><div class="flex flex-wrap justify-center gap-3 sm:gap-4 mb-6 relative px-2"><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="Email"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M21.75 6.75v10.5a2.25 2.25 0 0 1-2.25 2.25h-15a2.25 2.25 0 0 1-2.25-2.25V6.75m19.5 0A2.25 2.25 0 0 0 19.5 4.5h-15a2.25 2.25 0 0 0-2.25 2.25m19.5 0v.243a2.25 2.25 0 0 1-1.07 1.916l-7.5 4.615a2.25 2.25 0 0 1-2.36 0L3.32 8.91a2.25 2.25 0 0 1-1.07-1.916V6.75"></path></svg></button></div><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="Location"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M15 10.5a3 3 0 1 1-6 0 3 3 0 0 1 6 0Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 10.5c0 7.142-7.5 11.25-7.5 11.25S4.5 17.642 4.5 10.5a7.5 7.5 0 1 1 15 0Z"></path></svg></button></div><a href="https://scholar.google.com/citations?hl=zh-CN&amp;user=n3VYo4YAAAAJ" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="Google Scholar"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M4.26 10.147a60.438 60.438 0 0 0-.491 6.347A48.62 48.62 0 0 1 12 20.904a48.62 48.62 0 0 1 8.232-4.41 60.46 60.46 0 0 0-.491-6.347m-15.482 0a50.636 50.636 0 0 0-2.658-.813A59.906 59.906 0 0 1 12 3.493a59.903 59.903 0 0 1 10.399 5.84c-.896.248-1.783.52-2.658.814m-15.482 0A50.717 50.717 0 0 1 12 13.489a50.702 50.702 0 0 1 7.74-3.342M6.75 15a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm0 0v-3.675A55.378 55.378 0 0 1 12 8.443m-7.007 11.55A5.981 5.981 0 0 0 6.75 15.75v-1.5"></path></svg></a><a href="https://orcid.org/0000-0002-9474-0135" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="ORCID"><svg viewBox="0 0 24 24" fill="currentColor" class="h-5 w-5" xmlns="http://www.w3.org/2000/svg"><path d="M12 0C5.372 0 0 5.372 0 12s5.372 12 12 12 12-5.372 12-12S18.628 0 12 0zM7.369 4.378c.525 0 .947.431.947.947s-.422.947-.947.947a.95.95 0 0 1-.947-.947c0-.525.422-.947.947-.947zm-.722 3.038h1.444v10.041H6.647V7.416zm3.562 0h3.9c3.712 0 5.344 2.653 5.344 5.025 0 2.578-2.016 5.025-5.325 5.025h-3.919V7.416zm1.444 1.303v7.444h2.297c3.272 0 4.022-2.484 4.022-3.722 0-2.016-1.284-3.722-4.097-3.722h-2.222z"></path></svg></a></div><div class="bg-neutral-100 dark:bg-neutral-800 rounded-lg p-4 mb-6 hover:shadow-lg transition-all duration-200 hover:scale-[1.02]"><h3 class="font-semibold text-primary mb-3">Research Interests</h3><div class="space-y-2 text-sm text-neutral-700 dark:text-neutral-500"><div>Brain-inspired Computing</div><div>Spiking Neural Network</div><div>Event Camera</div><div>Federated Learning</div></div></div><div class="flex justify-center"><div class="relative"><button class="flex items-center space-x-2 px-4 py-2 rounded-lg font-medium text-sm transition-all duration-200 bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-500 hover:bg-red-50 dark:hover:bg-red-900/20 hover:text-red-600 dark:hover:text-red-400 cursor-pointer" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-4 w-4"><path stroke-linecap="round" stroke-linejoin="round" d="M21 8.25c0-2.485-2.099-4.5-4.688-4.5-1.935 0-3.597 1.126-4.312 2.733-.715-1.607-2.377-2.733-4.313-2.733C5.1 3.75 3 5.765 3 8.25c0 7.22 9 12 9 12s9-4.78 9-12Z"></path></svg><span>Like</span></button></div></div></div></div><div class="lg:col-span-2 space-y-8"><section id="about" class="scroll-mt-24 space-y-8"><section style="opacity:0;transform:translateY(20px)"><h2 class="text-2xl font-serif font-bold text-primary mb-4">About</h2><div class="text-neutral-700 dark:text-neutral-600 leading-relaxed"><p class="mb-4 last:mb-0">I am a Lecturer at the Complex Laboratory of New Finace and Economics (<a href="https://nicelab.swufe.edu.cn/index.htm" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">NiceLab</a>), Southwestern University of Finance and Economics (SWUFE).</p>
<p class="mb-4 last:mb-0">Prior to this, I obtained a BSc degree with in Information and Computing Science from the Yantai University, in 2017, and the Ph.D. degree in computer science from the University of Electronic Science and Technology of China, in 2024, advised by <a href="https://nicelab.swufe.edu.cn/info/1013/1170.htm" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Prof. Guisong Liu</a>.</p>
<p class="mb-4 last:mb-0">My current research focuses on investigating  in brain-inspired computing, specifically, event-camera based multimodal visual fusion, as well as the spiking neural network models and their learning mechanisms.</p></div></section><section style="opacity:0;transform:translateY(20px)"><div class="flex items-center justify-between mb-4"><h2 class="text-2xl font-serif font-bold text-primary">Selected Publications</h2><a class="text-accent hover:text-accent-dark text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm" href="/publications/">View All â†’</a></div><div class="space-y-4"><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">SFedCA: Credit Assignment-Based Active Client Selection Strategy for Spiking Federated Learning</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent ">Qiugang Zhan</span>, </span><span><span class=" ">Jinbo Cao</span>, </span><span><span class=" ">Xiurui Xie</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">â€ </sup>, </span><span><span class=" ">Huajin Tang</span>, </span><span><span class=" ">Malu Zhang</span>, </span><span><span class=" ">Shantian Yang</span>, </span><span><span class=" ">Guisong Liu</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">â€ </sup></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">IEEE Transactions on Neural Networks and Learning Systems</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">xxx</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">A two-stage spiking meta-learning method for few-shot classification</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent ">Qiugang Zhan</span>, </span><span><span class=" ">Bingchao Wang</span>, </span><span><span class=" ">Anning Jiang</span>, </span><span><span class=" ">Xiurui Xie</span>, </span><span><span class=" ">Malu Zhang</span>, </span><span><span class=" ">Guisong Liu</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">â€ </sup></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">Knowledge-Based Systems</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">xxx</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Spiking Transfer Learning From RGB Image to Neuromorphic Event Stream</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent ">Qiugang Zhan</span>, </span><span><span class=" ">Guisong Liu</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">â€ </sup>, </span><span><span class=" ">Xiurui Xie</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">â€ </sup>, </span><span><span class=" ">Ran Tao</span>, </span><span><span class=" ">Malu Zhang</span>, </span><span><span class=" ">Huajin Tang</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">IEEE Transactions on Image Processing</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">xxx</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Bio-inspired Active Learning method in spiking neural network</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent ">Qiugang Zhan</span>, </span><span><span class=" ">Guisong Liu</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">â€ </sup>, </span><span><span class=" ">Xiurui Xie</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">â€ </sup>, </span><span><span class=" ">Malu Zhang</span>, </span><span><span class=" ">Guolin Sun</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">Knowledge-Based Systems</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">xxx</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Effective Transfer Learning Algorithm in Spiking Neural Networks</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent ">Qiugang Zhan</span>, </span><span><span class=" ">Guisong Liu</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">â€ </sup>, </span><span><span class=" ">Xiurui Xie</span>, </span><span><span class=" ">Guolin Sun</span>, </span><span><span class=" ">Huajin Tang</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">IEEE Transactions on Cybernetics</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">xxx</p></div></div></section><section style="opacity:0;transform:translateY(20px)"><h2 class="text-2xl font-serif font-bold text-primary mb-4">News</h2><div class="space-y-3"><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2026-02</span><p class="text-sm text-neutral-700">Our work has been accepted by CVPR 2026! ðŸŽ‰</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-12</span><p class="text-sm text-neutral-700">Our work has been accepted by IEEE TNNLS! ðŸŽ‰</p></div></div></section></section></div></div></div><!--$--><!--/$--><!--$--><!--/$--></main><footer class="border-t border-neutral-200/50 bg-neutral-50/50 dark:bg-neutral-900/50 dark:border-neutral-700/50"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-6"><div class="flex flex-col sm:flex-row justify-between items-center gap-2"><p class="text-xs text-neutral-500">Last updated: <!-- -->February 21, 2026</p><p class="text-xs text-neutral-500 flex items-center"><a href="https://github.com/xyjoey/PRISM" target="_blank" rel="noopener noreferrer">Built with PRISM</a><span class="ml-2">ðŸš€</span></p></div></div></footer></div><script src="/_next/static/chunks/webpack-af78f5c2c11af041.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[3719,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-a118809a65dd8e19.js\"],\"ThemeProvider\"]\n3:I[768,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-a118809a65dd8e19.js\"],\"default\"]\n4:I[7555,[],\"\"]\n5:I[1295,[],\"\"]\n6:I[2548,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-a118809a65dd8e19.js\"],\"default\"]\n7:I[7437,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"178\",\"static/chunks/178-595a94b9af1e67b5.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"748\",\"static/chunks/748-b0753eeaf12cb586.js\",\"974\",\"static/chunks/app/page-31460eb0e55869a4.js\"],\"default\"]\n8:I[9507,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"178\",\"static/chunks/178-595a94b9af1e67b5.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"748\",\"static/chunks/748-b0753eeaf12cb586.js\",\"974\",\"static/chunks/app/page-31460eb0e55869a4.js\"],\"default\"]\n9:I[5218,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"178\",\"static/chunks/178-595a94b9af1e67b5.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"748\",\"static/chunks/748-b0753eeaf12cb586.js\",\"974\",\"static/chunks/app/page-31460eb0e55869a4.js\"],\"default\"]\n14:I[1990,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"178\",\"static/chunks/178-595a94b9af1e67b5.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"748\",\"static/chunks/748-b0753eeaf12cb586.js\",\"974\",\"static/chunks/app/page-31460eb0e55869a4.js\"],\"default\"]\n15:I[9665,[],\"MetadataBoundary\"]\n17:I[9665,[],\"OutletBoundary\"]\n1a:I[4911,[],\"AsyncMetadataOutlet\"]\n1c:I[9665,[],\"ViewportBoundary\"]\n1e:I[6614,[],\"\"]\n:HL[\"/_next/static/css/4c59616ffa01abc8.css\",\"style\"]\na:T4f2,The spiking federated learning (FL) is an emerging distributed learning paradigm that allows resource-constrained devices to "])</script><script>self.__next_f.push([1,"train collaboratively at low power consumption without exchanging local data. It takes advantage of both the privacy computation property in FL and the energy efficiency in spiking neural networks (SNNs). However, existing spiking FL methods employ a random selection approach for client aggregation, assuming unbiased client participation. This neglect of statistical heterogeneity significantly affects the convergence and precision of the global model. In this work, we propose a credit assignment-based active client selection strategy for spiking federated learning, the SFedCA, to aggregate clients contributing to the global sample distribution balance judiciously. Specifically, the client credits are assigned by the firing intensity state before and after local model training, which reflects the difference in local data distribution from the global model. The comprehensive experiments are conducted on various non-identical and independent distribution (non-IID) scenarios. The experimental results demonstrate that the SFedCA outperforms the existing state-of-the-art spiking FL methods and requires fewer communication rounds.b:T6cf,@ARTICLE{zhan2025sfedca,\n  author = {Zhan, Qiugang and Cao, Jinbo and Xie, Xiurui and Tang, Huajin and Zhang, Malu and Yang, Shantian and Liu, Guisong},\n  journal = {IEEE Transactions on Neural Networks and Learning Systems},\n  title = {SFedCA: Credit Assignment-Based Active Client Selection Strategy for Spiking Federated Learning},\n  year = {2025},\n  volume = {},\n  number = {},\n  pages = {1-13},\n  publisher = {IEEE},\n  abstract = {The spiking federated learning (FL) is an emerging distributed learning paradigm that allows resource-constrained devices to train collaboratively at low power consumption without exchanging local data. It takes advantage of both the privacy computation property in FL and the energy efficiency in spiking neural networks (SNNs). However, existing spiking FL methods employ a random selection approach for client aggregation, assuming unbiased client participation"])</script><script>self.__next_f.push([1,". This neglect of statistical heterogeneity significantly affects the convergence and precision of the global model. In this work, we propose a credit assignment-based active client selection strategy for spiking federated learning, the SFedCA, to aggregate clients contributing to the global sample distribution balance judiciously. Specifically, the client credits are assigned by the firing intensity state before and after local model training, which reflects the difference in local data distribution from the global model. The comprehensive experiments are conducted on various non-identical and independent distribution (non-IID) scenarios. The experimental results demonstrate that the SFedCA outperforms the existing state-of-the-art spiking FL methods and requires fewer communication rounds.},\n  doi = {10.1109/TNNLS.2025.3639578}\n}c:T549,In recent years, deep spiking neural networks (SNNs) have demonstrated promising performance across various applications, owing to their low-power characteristics. Research on SNN meta-learning has enabled SNNs to reduce both label cost and computational power consumption in few-shot classification tasks. However, current SNN meta-learning methods still lag behind traditional artificial neural networks (ANNs) in terms of accuracy. In this work, we explore a two-stage metric-based SNN meta-learning framework that achieves the highest accuracy performance in SNN. This framework comprises a pre-training stage and a meta-training stage. During pre-training, a classification embedding SNN model (CESM) is trained to extract image features. Subsequently, in the meta-training stage, the meta embedding SNN model (MESM) employs the centered kernel alignment (CKA) method to measure the similarity between these learned features for meta-learning. We conduct extensive experiments on the Omniglot, tieredImageNet, and miniImageNet datasets, evaluating both CESM and MESM models. Experimental results demonstrate that the proposed framework improves performance by 5% on average compared to previou"])</script><script>self.__next_f.push([1,"s SNN meta-learning approaches. The proposed method surpasses the early classical ANN methods and further closes the gap with ANN state-of-the-art methods.d:T732,@article{ZHAN2024111220,\n  title = {A two-stage spiking meta-learning method for few-shot classification},\n  journal = {Knowledge-Based Systems},\n  volume = {284},\n  pages = {111220},\n  year = {2024},\n  issn = {0950-7051},\n  doi = {https://doi.org/10.1016/j.knosys.2023.111220},\n  url = {https://www.sciencedirect.com/science/article/pii/S095070512300970X},\n  author = {Zhan, Qiugang and Wang, Bingchao and Jiang, Anning and Xie, Xiurui and Zhang, Malu and Liu, Guisong},\n  abstract = {In recent years, deep spiking neural networks (SNNs) have demonstrated promising performance across various applications, owing to their low-power characteristics. Research on SNN meta-learning has enabled SNNs to reduce both label cost and computational power consumption in few-shot classification tasks. However, current SNN meta-learning methods still lag behind traditional artificial neural networks (ANNs) in terms of accuracy. In this work, we explore a two-stage metric-based SNN meta-learning framework that achieves the highest accuracy performance in SNN. This framework comprises a pre-training stage and a meta-training stage. During pre-training, a classification embedding SNN model (CESM) is trained to extract image features. Subsequently, in the meta-training stage, the meta embedding SNN model (MESM) employs the centered kernel alignment (CKA) method to measure the similarity between these learned features for meta-learning. We conduct extensive experiments on the Omniglot, tieredImageNet, and miniImageNet datasets, evaluating both CESM and MESM models. Experimental results demonstrate that the proposed framework improves performance by 5% on average compared to previous SNN meta-learning approaches. The proposed method surpasses the early classical ANN methods and further closes the gap with ANN state-of-the-art methods.}\n}e:T654,Recent advances in bio-inspired visio"])</script><script>self.__next_f.push([1,"n with event cameras and associated spiking neural networks (SNNs) have provided promising solutions for low-power consumption neuromorphic tasks. However, as the research of event cameras is still in its infancy, the amount of labeled event stream data is much less than that of the RGB database. The traditional method of converting static images into event streams by simulation to increase the sample size cannot simulate the characteristics of event cameras such as high temporal resolution. To take advantage of both the rich knowledge in labeled RGB images and the features of the event camera, we propose a transfer learning method from the RGB to the event domain in this paper. Specifically, we first introduce a transfer learning framework named R2ETL (RGB to Event Transfer Learning), including a novel encoding alignment module and a feature alignment module. Then, we introduce the temporal centered kernel alignment (TCKA) loss function to improve the efficiency of transfer learning. It aligns the distribution of temporal neuron states by adding a temporal learning constraint. Finally, we theoretically analyze the amount of data required by the deep neuromorphic model to prove the necessity of our method. Numerous experiments demonstrate that our proposed framework outperforms the state-of-the-art SNN and artificial neural network (ANN) models trained on event streams, including N-MNIST, CIFAR10-DVS and N-Caltech101. This indicates that the R2ETL framework is able to leverage the knowledge of labeled RGB images to help the training of SNN on event streams.f:T7f4,@ARTICLE{zhan2024spiking,\n  author = {Zhan, Qiugang and Liu, Guisong and Xie, Xiurui and Tao, Ran and Zhang, Malu and Tang, Huajin},\n  journal = {IEEE Transactions on Image Processing},\n  title = {Spiking Transfer Learning From RGB Image to Neuromorphic Event Stream},\n  year = {2024},\n  volume = {33},\n  number = {},\n  pages = {4274-4287},\n  publisher = {IEEE},\n  abstract = {Recent advances in bio-inspired vision with event cameras and associated spiking "])</script><script>self.__next_f.push([1,"neural networks (SNNs) have provided promising solutions for low-power consumption neuromorphic tasks. However, as the research of event cameras is still in its infancy, the amount of labeled event stream data is much less than that of the RGB database. The traditional method of converting static images into event streams by simulation to increase the sample size cannot simulate the characteristics of event cameras such as high temporal resolution. To take advantage of both the rich knowledge in labeled RGB images and the features of the event camera, we propose a transfer learning method from the RGB to the event domain in this paper. Specifically, we first introduce a transfer learning framework named R2ETL (RGB to Event Transfer Learning), including a novel encoding alignment module and a feature alignment module. Then, we introduce the temporal centered kernel alignment (TCKA) loss function to improve the efficiency of transfer learning. It aligns the distribution of temporal neuron states by adding a temporal learning constraint. Finally, we theoretically analyze the amount of data required by the deep neuromorphic model to prove the necessity of our method. Numerous experiments demonstrate that our proposed framework outperforms the state-of-the-art SNN and artificial neural network (ANN) models trained on event streams, including N-MNIST, CIFAR10-DVS and N-Caltech101. This indicates that the R2ETL framework is able to leverage the knowledge of labeled RGB images to help the training of SNN on event streams.},\n  doi = {10.1109/TIP.2024.3430043}\n}10:T453,Spiking neural networks (SNNs) have gained a lot of attention and achievements recently because of their low-power advantages on neuromorphic hardware. However, training deep SNNs still requires a large number of labeled data which are expensive to obtain. To address this issue, we propose an effective Bio-inspired Active Learning (BAL) method in this paper to reduce the training cost of SNN models. Specifically, bio-inspired behavior patterns of spiking neu"])</script><script>self.__next_f.push([1,"rons are defined to represent the internal states of SNN models for active learning. Then, an active learning sample selection strategy is proposed by leveraging the empirical and generalization pattern divergence in SNNs. By labeling selected samples and adding them to training, behavioral patterns can be optimized to improve the performance of neural networks. Comprehensive experiments are conducted on the CIFAR-10, SVHN, and Fashion-MNIST datasets with various sample proportions. The experimental results demonstrate that the proposed BAL achieves state-of-the-arts performance in SNNs compared with the existing active learning methods.11:T620,@article{ZHAN2023110193,\n  title = {Bio-inspired Active Learning method in spiking neural network},\n  journal = {Knowledge-Based Systems},\n  volume = {261},\n  pages = {110193},\n  year = {2023},\n  issn = {0950-7051},\n  doi = {https://doi.org/10.1016/j.knosys.2022.110193},\n  url = {https://www.sciencedirect.com/science/article/pii/S0950705122012898},\n  author = {Zhan, Qiugang and Liu, Guisong and Xie, Xiurui and Zhang, Malu and Sun, Guolin},\n  abstract = {Spiking neural networks (SNNs) have gained a lot of attention and achievements recently because of their low-power advantages on neuromorphic hardware. However, training deep SNNs still requires a large number of labeled data which are expensive to obtain. To address this issue, we propose an effective Bio-inspired Active Learning (BAL) method in this paper to reduce the training cost of SNN models. Specifically, bio-inspired behavior patterns of spiking neurons are defined to represent the internal states of SNN models for active learning. Then, an active learning sample selection strategy is proposed by leveraging the empirical and generalization pattern divergence in SNNs. By labeling selected samples and adding them to training, behavioral patterns can be optimized to improve the performance of neural networks. Comprehensive experiments are conducted on the CIFAR-10, SVHN, and Fashion-MNIST datasets with various sample "])</script><script>self.__next_f.push([1,"proportions. The experimental results demonstrate that the proposed BAL achieves state-of-the-arts performance in SNNs compared with the existing active learning methods.}\n}12:T42f,As the third generation of neural networks, spiking neural networks (SNNs) have gained much attention recently because of their high energy efficiency on neuromorphic hardware. However, training deep SNNs requires many labeled data that are expensive to obtain in real-world applications, as traditional artificial neural networks (ANNs). In order to address this issue, transfer learning has been proposed and widely used in traditional ANNs, but it has limited use in SNNs. In this article, we propose an effective transfer learning framework for deep SNNs based on the domain in-variance representation. Specifically, we analyze the rationality of centered kernel alignment (CKA) as a domain distance measurement relative to maximum mean discrepancy (MMD) in deep SNNs. In addition, we study the feature transferability across different layers by testing on the Office-31, Office-Caltech-10, and PACS datasets. The experimental results demonstrate the transferability of SNNs and show the effectiveness of the proposed transfer learning framework by using CKA in SNNs.13:T5bf,@ARTICLE{zhan2022effective,\n  author = {Zhan, Qiugang and Liu, Guisong and Xie, Xiurui and Sun, Guolin and Tang, Huajin},\n  journal = {IEEE Transactions on Cybernetics},\n  title = {Effective Transfer Learning Algorithm in Spiking Neural Networks},\n  year = {2022},\n  volume = {52},\n  number = {12},\n  pages = {13323-13335},\n  publisher = {IEEE},\n  abstract = {As the third generation of neural networks, spiking neural networks (SNNs) have gained much attention recently because of their high energy efficiency on neuromorphic hardware. However, training deep SNNs requires many labeled data that are expensive to obtain in real-world applications, as traditional artificial neural networks (ANNs). In order to address this issue, transfer learning has been proposed and widely used in tr"])</script><script>self.__next_f.push([1,"aditional ANNs, but it has limited use in SNNs. In this article, we propose an effective transfer learning framework for deep SNNs based on the domain in-variance representation. Specifically, we analyze the rationality of centered kernel alignment (CKA) as a domain distance measurement relative to maximum mean discrepancy (MMD) in deep SNNs. In addition, we study the feature transferability across different layers by testing on the Office-31, Office-Caltech-10, and PACS datasets. The experimental results demonstrate the transferability of SNNs and show the effectiveness of the proposed transfer learning framework by using CKA in SNNs.},\n  doi = {10.1109/TCYB.2021.3079097}\n}"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"41aUwkjzsenCq-WfxNfCc\",\"p\":\"\",\"c\":[\"\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/4c59616ffa01abc8.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/favicon.svg\",\"type\":\"image/svg+xml\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://google-fonts.jialeliu.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://google-fonts.jialeliu.com\",\"crossOrigin\":\"\"}],[\"$\",\"link\",null,{\"rel\":\"preload\",\"as\":\"style\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}],[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"id\":\"gfonts-css\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\",\"media\":\"print\"}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              (function(){\\n                var l = document.getElementById('gfonts-css');\\n                if (!l) return;\\n                if (l.media !== 'all') {\\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\\n                }\\n              })();\\n            \"}}],[\"$\",\"noscript\",null,{\"children\":[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}]}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              try {\\n                const theme = localStorage.getItem('theme-storage');\\n                const parsed = theme ? JSON.parse(theme) : null;\\n                const setting = parsed?.state?.theme || 'system';\\n                const prefersDark = typeof window !== 'undefined' \u0026\u0026 window.matchMedia \u0026\u0026 window.matchMedia('(prefers-color-scheme: dark)').matches;\\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\\n                var root = document.documentElement;\\n                root.classList.add(effective);\\n                root.setAttribute('data-theme', effective);\\n              } catch (e) {\\n                var root = document.documentElement;\\n                root.classList.add('light');\\n                root.setAttribute('data-theme', 'light');\\n              }\\n            \"}}]]}],[\"$\",\"body\",null,{\"className\":\"font-sans antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[[\"$\",\"$L3\",null,{\"items\":[{\"title\":\"About\",\"type\":\"page\",\"target\":\"about\",\"href\":\"/\"},{\"title\":\"Publications\",\"type\":\"page\",\"target\":\"publications\",\"href\":\"/publications\"},{\"title\":\"Teaching\",\"type\":\"page\",\"target\":\"teaching\",\"href\":\"/teaching\"},{\"title\":\"Services\",\"type\":\"page\",\"target\":\"services\",\"href\":\"/services\"},{\"title\":\"CV\",\"type\":\"page\",\"target\":\"cv\",\"href\":\"/cv\"}],\"siteTitle\":\"Qiugang Zhan\",\"enableOnePageMode\":false}],[\"$\",\"main\",null,{\"className\":\"min-h-screen pt-16 lg:pt-20\",\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"$L6\",null,{\"lastUpdated\":\"February 21, 2026\"}]]}]}]]}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen\",\"children\":[\"$\",\"div\",null,{\"className\":\"grid grid-cols-1 lg:grid-cols-3 gap-12\",\"children\":[[\"$\",\"div\",null,{\"className\":\"lg:col-span-1\",\"children\":[\"$\",\"$L7\",null,{\"author\":{\"name\":\"Qiugang Zhan\",\"title\":\"PhD\",\"institution\":\"Southwestern University of Finance and Economics (SWUFE)\",\"avatar\":\"/bio.jpg\"},\"social\":{\"email\":\"zhanqg@swufe.edu.cn\",\"location\":\"Chengdu, China\",\"location_details\":[\"555, Liutai Avenue, Chengdu , China\"],\"google_scholar\":\"https://scholar.google.com/citations?hl=zh-CN\u0026user=n3VYo4YAAAAJ\",\"orcid\":\"https://orcid.org/0000-0002-9474-0135\"},\"features\":{\"enable_likes\":true,\"enable_one_page_mode\":false},\"researchInterests\":[\"Brain-inspired Computing\",\"Spiking Neural Network\",\"Event Camera\",\"Federated Learning\"]}]}],[\"$\",\"div\",null,{\"className\":\"lg:col-span-2 space-y-8\",\"children\":[[\"$\",\"section\",\"about\",{\"id\":\"about\",\"className\":\"scroll-mt-24 space-y-8\",\"children\":[[[\"$\",\"$L8\",\"about\",{\"content\":\"I am a Lecturer at the Complex Laboratory of New Finace and Economics ([NiceLab](https://nicelab.swufe.edu.cn/index.htm)), Southwestern University of Finance and Economics (SWUFE).\\n\\nPrior to this, I obtained a BSc degree with in Information and Computing Science from the Yantai University, in 2017, and the Ph.D. degree in computer science from the University of Electronic Science and Technology of China, in 2024, advised by [Prof. Guisong Liu](https://nicelab.swufe.edu.cn/info/1013/1170.htm). \\n\\nMy current research focuses on investigating  in brain-inspired computing, specifically, event-camera based multimodal visual fusion, as well as the spiking neural network models and their learning mechanisms.\",\"title\":\"About\"}],[\"$\",\"$L9\",\"featured_publications\",{\"publications\":[{\"id\":\"zhan2025sfedca\",\"title\":\"SFedCA: Credit Assignment-Based Active Client Selection Strategy for Spiking Federated Learning\",\"authors\":[{\"name\":\"Qiugang Zhan\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jinbo Cao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiurui Xie\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Huajin Tang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Malu Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Shantian Yang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guisong Liu\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false}],\"year\":2025,\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"Neurons;Data models;Training;Firing;Servers;Convergence;Membrane potentials;Biological system modeling;Federated learning;Adaptation models;Data heterogeneity;federated learning (FL);selection strategy;spiking neural network (SNN)\"],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:0:tags\",\"researchArea\":\"neural-networks\",\"journal\":\"IEEE Transactions on Neural Networks and Learning Systems\",\"conference\":\"\",\"volume\":\"\",\"issue\":\"\",\"pages\":\"1-13\",\"doi\":\"10.1109/TNNLS.2025.3639578\",\"code\":\"\",\"abstract\":\"$a\",\"description\":\"xxx\",\"selected\":true,\"preview\":\"example.png\",\"bibtex\":\"$b\"},{\"id\":\"ZHAN2024111220\",\"title\":\"A two-stage spiking meta-learning method for few-shot classification\",\"authors\":[{\"name\":\"Qiugang Zhan\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Bingchao Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Anning Jiang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiurui Xie\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Malu Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guisong Liu\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false}],\"year\":2024,\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"Spiking neural networks\",\"Meta-learning\",\"Centered Kernel alignment\",\"Few-shot classification\"],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:1:tags\",\"researchArea\":\"neural-networks\",\"journal\":\"Knowledge-Based Systems\",\"conference\":\"\",\"volume\":\"284\",\"pages\":\"111220\",\"doi\":\"https://doi.org/10.1016/j.knosys.2023.111220\",\"url\":\"https://www.sciencedirect.com/science/article/pii/S095070512300970X\",\"code\":\"\",\"abstract\":\"$c\",\"description\":\"xxx\",\"selected\":true,\"preview\":\"example.png\",\"bibtex\":\"$d\"},{\"id\":\"zhan2024spiking\",\"title\":\"Spiking Transfer Learning From RGB Image to Neuromorphic Event Stream\",\"authors\":[{\"name\":\"Qiugang Zhan\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guisong Liu\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Xiurui Xie\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Ran Tao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Malu Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Huajin Tang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"Transfer learning;Cameras;Streaming media;Data models;Training;Loss measurement;Brightness;Spiking neural networks;transfer learning;event camera\"],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:2:tags\",\"researchArea\":\"neural-networks\",\"journal\":\"IEEE Transactions on Image Processing\",\"conference\":\"\",\"volume\":\"33\",\"issue\":\"\",\"pages\":\"4274-4287\",\"doi\":\"10.1109/TIP.2024.3430043\",\"code\":\"\",\"abstract\":\"$e\",\"description\":\"xxx\",\"selected\":true,\"preview\":\"example.png\",\"bibtex\":\"$f\"},{\"id\":\"ZHAN2023110193\",\"title\":\"Bio-inspired Active Learning method in spiking neural network\",\"authors\":[{\"name\":\"Qiugang Zhan\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guisong Liu\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Xiurui Xie\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Malu Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guolin Sun\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2023,\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"Spiking neural networks\",\"Active learning\",\"Bio-inspired behavior patterns\"],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:3:tags\",\"researchArea\":\"neural-networks\",\"journal\":\"Knowledge-Based Systems\",\"conference\":\"\",\"volume\":\"261\",\"pages\":\"110193\",\"doi\":\"https://doi.org/10.1016/j.knosys.2022.110193\",\"url\":\"https://www.sciencedirect.com/science/article/pii/S0950705122012898\",\"code\":\"\",\"abstract\":\"$10\",\"description\":\"xxx\",\"selected\":true,\"preview\":\"example.png\",\"bibtex\":\"$11\"},{\"id\":\"zhan2022effective\",\"title\":\"Effective Transfer Learning Algorithm in Spiking Neural Networks\",\"authors\":[{\"name\":\"Qiugang Zhan\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guisong Liu\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Xiurui Xie\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guolin Sun\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Huajin Tang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2022,\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"Transfer learning;Deep learning;Neurons;Kernel;Feature extraction;Artificial neural networks;Membrane potentials;Centered kernel alignment;deep learning;spiking neural network (SNN);transfer learning\"],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:4:tags\",\"researchArea\":\"neural-networks\",\"journal\":\"IEEE Transactions on Cybernetics\",\"conference\":\"\",\"volume\":\"52\",\"issue\":\"12\",\"pages\":\"13323-13335\",\"doi\":\"10.1109/TCYB.2021.3079097\",\"code\":\"https://github.com/QgZhan/SNNTL\",\"abstract\":\"$12\",\"description\":\"xxx\",\"selected\":true,\"preview\":\"example.png\",\"bibtex\":\"$13\"}],\"title\":\"Selected Publications\",\"enableOnePageMode\":false}],[\"$\",\"$L14\",\"news\",{\"items\":[{\"date\":\"2026-02\",\"content\":\"Our work has been accepted by CVPR 2026! ðŸŽ‰\"},{\"date\":\"2025-12\",\"content\":\"Our work has been accepted by IEEE TNNLS! ðŸŽ‰\"}],\"title\":\"News\"}]],false,false,false]}]]}]]}]}],[\"$\",\"$L15\",null,{\"children\":\"$L16\"}],null,[\"$\",\"$L17\",null,{\"children\":[\"$L18\",\"$L19\",[\"$\",\"$L1a\",null,{\"promise\":\"$@1b\"}]]}]]}],{},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"dJ7rqdJ58OnZNpr3TL8o0\",{\"children\":[[\"$\",\"$L1c\",null,{\"children\":\"$L1d\"}],null]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$1e\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"1f:\"$Sreact.suspense\"\n20:I[4911,[],\"AsyncMetadata\"]\n16:[\"$\",\"$1f\",null,{\"fallback\":null,\"children\":[\"$\",\"$L20\",null,{\"promise\":\"$@21\"}]}]\n"])</script><script>self.__next_f.push([1,"19:null\n"])</script><script>self.__next_f.push([1,"1d:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n18:null\n"])</script><script>self.__next_f.push([1,"21:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Qiugang Zhan\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Lecturer at the Southwestern University of Finance and Economics (SWUFE).\"}],[\"$\",\"meta\",\"2\",{\"name\":\"author\",\"content\":\"Qiugang Zhan\"}],[\"$\",\"meta\",\"3\",{\"name\":\"keywords\",\"content\":\"Qiugang Zhan,PhD,Research,Southwestern University of Finance and Economics (SWUFE)\"}],[\"$\",\"meta\",\"4\",{\"name\":\"creator\",\"content\":\"Qiugang Zhan\"}],[\"$\",\"meta\",\"5\",{\"name\":\"publisher\",\"content\":\"Qiugang Zhan\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"Qiugang Zhan\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"Lecturer at the Southwestern University of Finance and Economics (SWUFE).\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:site_name\",\"content\":\"Qiugang Zhan's Academic Website\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:title\",\"content\":\"Qiugang Zhan\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:description\",\"content\":\"Lecturer at the Southwestern University of Finance and Economics (SWUFE).\"}],[\"$\",\"link\",\"14\",{\"rel\":\"icon\",\"href\":\"/favicon.svg\"}]],\"error\":null,\"digest\":\"$undefined\"}\n1b:{\"metadata\":\"$21:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>