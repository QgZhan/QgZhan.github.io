1:"$Sreact.fragment"
2:I[3719,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-a118809a65dd8e19.js"],"ThemeProvider"]
3:I[768,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-a118809a65dd8e19.js"],"default"]
4:I[7555,[],""]
5:I[1295,[],""]
6:I[2548,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-a118809a65dd8e19.js"],"default"]
8:I[9665,[],"MetadataBoundary"]
a:I[9665,[],"OutletBoundary"]
d:I[4911,[],"AsyncMetadataOutlet"]
f:I[9665,[],"ViewportBoundary"]
11:I[6614,[],""]
:HL["/_next/static/css/4c59616ffa01abc8.css","style"]
0:{"P":null,"b":"k-ps3BQe1Ib6TwtNC5ngO","p":"","c":["","publications",""],"i":false,"f":[[["",{"children":[["slug","publications","d"],{"children":["__PAGE__",{}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/4c59616ffa01abc8.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"scroll-smooth","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","href":"/favicon.svg","type":"image/svg+xml"}],["$","link",null,{"rel":"dns-prefetch","href":"https://google-fonts.jialeliu.com"}],["$","link",null,{"rel":"preconnect","href":"https://google-fonts.jialeliu.com","crossOrigin":""}],["$","link",null,{"rel":"preload","as":"style","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}],["$","link",null,{"rel":"stylesheet","id":"gfonts-css","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap","media":"print"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              (function(){\n                var l = document.getElementById('gfonts-css');\n                if (!l) return;\n                if (l.media !== 'all') {\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\n                }\n              })();\n            "}}],["$","noscript",null,{"children":["$","link",null,{"rel":"stylesheet","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}]}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              try {\n                const theme = localStorage.getItem('theme-storage');\n                const parsed = theme ? JSON.parse(theme) : null;\n                const setting = parsed?.state?.theme || 'system';\n                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\n                var root = document.documentElement;\n                root.classList.add(effective);\n                root.setAttribute('data-theme', effective);\n              } catch (e) {\n                var root = document.documentElement;\n                root.classList.add('light');\n                root.setAttribute('data-theme', 'light');\n              }\n            "}}]]}],["$","body",null,{"className":"font-sans antialiased","children":["$","$L2",null,{"children":[["$","$L3",null,{"items":[{"title":"About","type":"page","target":"about","href":"/"},{"title":"Publications","type":"page","target":"publications","href":"/publications"},{"title":"Teaching","type":"page","target":"teaching","href":"/teaching"},{"title":"Fundings","type":"page","target":"fundings","href":"/fundings"}],"siteTitle":"Qiugang Zhan","enableOnePageMode":false}],["$","main",null,{"className":"min-h-screen pt-16 lg:pt-20","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","$L6",null,{"lastUpdated":"February 21, 2026"}]]}]}]]}]]}],{"children":[["slug","publications","d"],["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L7",["$","$L8",null,{"children":"$L9"}],null,["$","$La",null,{"children":["$Lb","$Lc",["$","$Ld",null,{"promise":"$@e"}]]}]]}],{},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","e81HsDUKAA6MIYGh1_dl7",{"children":[["$","$Lf",null,{"children":"$L10"}],null]}],null]}],false]],"m":"$undefined","G":["$11","$undefined"],"s":false,"S":true}
12:"$Sreact.suspense"
13:I[4911,[],"AsyncMetadata"]
15:I[6669,["17","static/chunks/17-9c04c9413a9f9f1f.js","178","static/chunks/178-595a94b9af1e67b5.js","748","static/chunks/748-b0753eeaf12cb586.js","182","static/chunks/app/%5Bslug%5D/page-f6d7032b5a4faf85.js"],"default"]
9:["$","$12",null,{"fallback":null,"children":["$","$L13",null,{"promise":"$@14"}]}]
16:T4f2,The spiking federated learning (FL) is an emerging distributed learning paradigm that allows resource-constrained devices to train collaboratively at low power consumption without exchanging local data. It takes advantage of both the privacy computation property in FL and the energy efficiency in spiking neural networks (SNNs). However, existing spiking FL methods employ a random selection approach for client aggregation, assuming unbiased client participation. This neglect of statistical heterogeneity significantly affects the convergence and precision of the global model. In this work, we propose a credit assignment-based active client selection strategy for spiking federated learning, the SFedCA, to aggregate clients contributing to the global sample distribution balance judiciously. Specifically, the client credits are assigned by the firing intensity state before and after local model training, which reflects the difference in local data distribution from the global model. The comprehensive experiments are conducted on various non-identical and independent distribution (non-IID) scenarios. The experimental results demonstrate that the SFedCA outperforms the existing state-of-the-art spiking FL methods and requires fewer communication rounds.17:T6cf,@ARTICLE{zhan2025sfedca,
  author = {Zhan, Qiugang and Cao, Jinbo and Xie, Xiurui and Tang, Huajin and Zhang, Malu and Yang, Shantian and Liu, Guisong},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  title = {SFedCA: Credit Assignment-Based Active Client Selection Strategy for Spiking Federated Learning},
  year = {2025},
  volume = {},
  number = {},
  pages = {1-13},
  publisher = {IEEE},
  abstract = {The spiking federated learning (FL) is an emerging distributed learning paradigm that allows resource-constrained devices to train collaboratively at low power consumption without exchanging local data. It takes advantage of both the privacy computation property in FL and the energy efficiency in spiking neural networks (SNNs). However, existing spiking FL methods employ a random selection approach for client aggregation, assuming unbiased client participation. This neglect of statistical heterogeneity significantly affects the convergence and precision of the global model. In this work, we propose a credit assignment-based active client selection strategy for spiking federated learning, the SFedCA, to aggregate clients contributing to the global sample distribution balance judiciously. Specifically, the client credits are assigned by the firing intensity state before and after local model training, which reflects the difference in local data distribution from the global model. The comprehensive experiments are conducted on various non-identical and independent distribution (non-IID) scenarios. The experimental results demonstrate that the SFedCA outperforms the existing state-of-the-art spiking FL methods and requires fewer communication rounds.},
  doi = {10.1109/TNNLS.2025.3639578}
}18:T510,Personalized federated learning (PFL) is a new paradigm to address the statistical heterogeneity problem in federated learning. Most existing PFL methods focus on leveraging global and local information such as model interpolation or parameter decoupling. However, these methods often overlook the generalization potential during local client learning. From a local optimization perspective, we propose a simple and general PFL method, Federated learning with Flexible Sharpness-Aware Minimization (FedFSA). Specifically, we emphasize the importance of applying a larger perturbation to critical layers of the local model when using the Sharpness-Aware Minimization (SAM) optimizer. Then, we design a metric, perturbation sensitivity, to estimate the layer-wise sharpness of each local model. Based on this metric, FedFSA can flexibly select the layers with the highest sharpness to employ larger perturbation. Extensive experiments are conducted on four datasets with two types of statistical heterogeneity for image classification. The results show that FedFSA outperforms seven state-of-the-art baselines by up to 8.26% in test accuracy. Besides, FedFSA can be applied to different model architectures and easily integrated into other federated learning methods, achieving a 4.45% improvement.19:T6bf,@inproceedings{xing2025flexible,
  title = {Flexible sharpness-aware personalized federated learning},
  author = {Xing, Xinda and Zhan(共一), Qiugang and Xie, Xiurui and Yang, Yuning and Wang, Qiang and Liu, Guisong},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {39},
  number = {20},
  pages = {21707--21715},
  year = {2025},
  abstract = {Personalized federated learning (PFL) is a new paradigm to address the statistical heterogeneity problem in federated learning. Most existing PFL methods focus on leveraging global and local information such as model interpolation or parameter decoupling. However, these methods often overlook the generalization potential during local client learning. From a local optimization perspective, we propose a simple and general PFL method, Federated learning with Flexible Sharpness-Aware Minimization (FedFSA). Specifically, we emphasize the importance of applying a larger perturbation to critical layers of the local model when using the Sharpness-Aware Minimization (SAM) optimizer. Then, we design a metric, perturbation sensitivity, to estimate the layer-wise sharpness of each local model. Based on this metric, FedFSA can flexibly select the layers with the highest sharpness to employ larger perturbation. Extensive experiments are conducted on four datasets with two types of statistical heterogeneity for image classification. The results show that FedFSA outperforms seven state-of-the-art baselines by up to 8.26% in test accuracy. Besides, FedFSA can be applied to different model architectures and easily integrated into other federated learning methods, achieving a 4.45% improvement.},
  doi = {10.1609/aaai.v39i20.35475}
}1a:T549,In recent years, deep spiking neural networks (SNNs) have demonstrated promising performance across various applications, owing to their low-power characteristics. Research on SNN meta-learning has enabled SNNs to reduce both label cost and computational power consumption in few-shot classification tasks. However, current SNN meta-learning methods still lag behind traditional artificial neural networks (ANNs) in terms of accuracy. In this work, we explore a two-stage metric-based SNN meta-learning framework that achieves the highest accuracy performance in SNN. This framework comprises a pre-training stage and a meta-training stage. During pre-training, a classification embedding SNN model (CESM) is trained to extract image features. Subsequently, in the meta-training stage, the meta embedding SNN model (MESM) employs the centered kernel alignment (CKA) method to measure the similarity between these learned features for meta-learning. We conduct extensive experiments on the Omniglot, tieredImageNet, and miniImageNet datasets, evaluating both CESM and MESM models. Experimental results demonstrate that the proposed framework improves performance by 5% on average compared to previous SNN meta-learning approaches. The proposed method surpasses the early classical ANN methods and further closes the gap with ANN state-of-the-art methods.1b:T732,@article{ZHAN2024111220,
  title = {A two-stage spiking meta-learning method for few-shot classification},
  journal = {Knowledge-Based Systems},
  volume = {284},
  pages = {111220},
  year = {2024},
  issn = {0950-7051},
  doi = {https://doi.org/10.1016/j.knosys.2023.111220},
  url = {https://www.sciencedirect.com/science/article/pii/S095070512300970X},
  author = {Zhan, Qiugang and Wang, Bingchao and Jiang, Anning and Xie, Xiurui and Zhang, Malu and Liu, Guisong},
  abstract = {In recent years, deep spiking neural networks (SNNs) have demonstrated promising performance across various applications, owing to their low-power characteristics. Research on SNN meta-learning has enabled SNNs to reduce both label cost and computational power consumption in few-shot classification tasks. However, current SNN meta-learning methods still lag behind traditional artificial neural networks (ANNs) in terms of accuracy. In this work, we explore a two-stage metric-based SNN meta-learning framework that achieves the highest accuracy performance in SNN. This framework comprises a pre-training stage and a meta-training stage. During pre-training, a classification embedding SNN model (CESM) is trained to extract image features. Subsequently, in the meta-training stage, the meta embedding SNN model (MESM) employs the centered kernel alignment (CKA) method to measure the similarity between these learned features for meta-learning. We conduct extensive experiments on the Omniglot, tieredImageNet, and miniImageNet datasets, evaluating both CESM and MESM models. Experimental results demonstrate that the proposed framework improves performance by 5% on average compared to previous SNN meta-learning approaches. The proposed method surpasses the early classical ANN methods and further closes the gap with ANN state-of-the-art methods.}
}1c:T654,Recent advances in bio-inspired vision with event cameras and associated spiking neural networks (SNNs) have provided promising solutions for low-power consumption neuromorphic tasks. However, as the research of event cameras is still in its infancy, the amount of labeled event stream data is much less than that of the RGB database. The traditional method of converting static images into event streams by simulation to increase the sample size cannot simulate the characteristics of event cameras such as high temporal resolution. To take advantage of both the rich knowledge in labeled RGB images and the features of the event camera, we propose a transfer learning method from the RGB to the event domain in this paper. Specifically, we first introduce a transfer learning framework named R2ETL (RGB to Event Transfer Learning), including a novel encoding alignment module and a feature alignment module. Then, we introduce the temporal centered kernel alignment (TCKA) loss function to improve the efficiency of transfer learning. It aligns the distribution of temporal neuron states by adding a temporal learning constraint. Finally, we theoretically analyze the amount of data required by the deep neuromorphic model to prove the necessity of our method. Numerous experiments demonstrate that our proposed framework outperforms the state-of-the-art SNN and artificial neural network (ANN) models trained on event streams, including N-MNIST, CIFAR10-DVS and N-Caltech101. This indicates that the R2ETL framework is able to leverage the knowledge of labeled RGB images to help the training of SNN on event streams.1d:T7f4,@ARTICLE{zhan2024spiking,
  author = {Zhan, Qiugang and Liu, Guisong and Xie, Xiurui and Tao, Ran and Zhang, Malu and Tang, Huajin},
  journal = {IEEE Transactions on Image Processing},
  title = {Spiking Transfer Learning From RGB Image to Neuromorphic Event Stream},
  year = {2024},
  volume = {33},
  number = {},
  pages = {4274-4287},
  publisher = {IEEE},
  abstract = {Recent advances in bio-inspired vision with event cameras and associated spiking neural networks (SNNs) have provided promising solutions for low-power consumption neuromorphic tasks. However, as the research of event cameras is still in its infancy, the amount of labeled event stream data is much less than that of the RGB database. The traditional method of converting static images into event streams by simulation to increase the sample size cannot simulate the characteristics of event cameras such as high temporal resolution. To take advantage of both the rich knowledge in labeled RGB images and the features of the event camera, we propose a transfer learning method from the RGB to the event domain in this paper. Specifically, we first introduce a transfer learning framework named R2ETL (RGB to Event Transfer Learning), including a novel encoding alignment module and a feature alignment module. Then, we introduce the temporal centered kernel alignment (TCKA) loss function to improve the efficiency of transfer learning. It aligns the distribution of temporal neuron states by adding a temporal learning constraint. Finally, we theoretically analyze the amount of data required by the deep neuromorphic model to prove the necessity of our method. Numerous experiments demonstrate that our proposed framework outperforms the state-of-the-art SNN and artificial neural network (ANN) models trained on event streams, including N-MNIST, CIFAR10-DVS and N-Caltech101. This indicates that the R2ETL framework is able to leverage the knowledge of labeled RGB images to help the training of SNN on event streams.},
  doi = {10.1109/TIP.2024.3430043}
}1e:T57a,In recent years, research on the federated Spiking Neural Network (SNN) has attracted increasing attention because of its advantages of low-power consumption and privacy security. However, existing federated SNN researches rely primarily on the Federated Average (FedAvg) strategy, transferring full network parameters between the server and clients and incurring substantial communication. To address this issue, we propose a Hint-layer Distillation-based Spiking Federated Learning (HDSFL) framework that reduces the communication cost by transferring knowledge and losslessly compressing the spiking tensor. To compensate for the information loss due to the binary representation of spikes in knowledge distillation, we introduce the hint-layer information instead of just soft-label distribution. To aggregate the knowledge effectively across clients on the server, we process the weighted knowledge aggregation on the spiking knowledge representation based on local performance. The experimental results on four classical and large-scale benchmarks show that our method reduces the communication cost by about 1–2 orders of magnitude compared to conventional methods, while achieving comparable accuracies. Especially on the CIFAR-10 dataset, our method achieves the same accuracy as FedAvg using only 20% of the communications, and consumes only 5.8% communications of FedAvg in a single round.1f:T78a,@article{xie2024federated,
  title = {Federated learning for spiking neural networks by hint-layer knowledge distillation},
  journal = {Applied Soft Computing},
  volume = {163},
  pages = {111901},
  year = {2024},
  issn = {1568-4946},
  publisher = {Elsevier},
  doi = {https://doi.org/10.1016/j.asoc.2024.111901},
  url = {https://www.sciencedirect.com/science/article/pii/S1568494624006756},
  author = {Xie, Xiurui and Feng, Jingxuan and Liu, Guisong and Zhan, Qiugang and Liu, Zhetong and Zhang, Malu},
  abstract = {In recent years, research on the federated Spiking Neural Network (SNN) has attracted increasing attention because of its advantages of low-power consumption and privacy security. However, existing federated SNN researches rely primarily on the Federated Average (FedAvg) strategy, transferring full network parameters between the server and clients and incurring substantial communication. To address this issue, we propose a Hint-layer Distillation-based Spiking Federated Learning (HDSFL) framework that reduces the communication cost by transferring knowledge and losslessly compressing the spiking tensor. To compensate for the information loss due to the binary representation of spikes in knowledge distillation, we introduce the hint-layer information instead of just soft-label distribution. To aggregate the knowledge effectively across clients on the server, we process the weighted knowledge aggregation on the spiking knowledge representation based on local performance. The experimental results on four classical and large-scale benchmarks show that our method reduces the communication cost by about 1–2 orders of magnitude compared to conventional methods, while achieving comparable accuracies. Especially on the CIFAR-10 dataset, our method achieves the same accuracy as FedAvg using only 20% of the communications, and consumes only 5.8% communications of FedAvg in a single round.}
}20:T453,Spiking neural networks (SNNs) have gained a lot of attention and achievements recently because of their low-power advantages on neuromorphic hardware. However, training deep SNNs still requires a large number of labeled data which are expensive to obtain. To address this issue, we propose an effective Bio-inspired Active Learning (BAL) method in this paper to reduce the training cost of SNN models. Specifically, bio-inspired behavior patterns of spiking neurons are defined to represent the internal states of SNN models for active learning. Then, an active learning sample selection strategy is proposed by leveraging the empirical and generalization pattern divergence in SNNs. By labeling selected samples and adding them to training, behavioral patterns can be optimized to improve the performance of neural networks. Comprehensive experiments are conducted on the CIFAR-10, SVHN, and Fashion-MNIST datasets with various sample proportions. The experimental results demonstrate that the proposed BAL achieves state-of-the-arts performance in SNNs compared with the existing active learning methods.21:T620,@article{ZHAN2023110193,
  title = {Bio-inspired Active Learning method in spiking neural network},
  journal = {Knowledge-Based Systems},
  volume = {261},
  pages = {110193},
  year = {2023},
  issn = {0950-7051},
  doi = {https://doi.org/10.1016/j.knosys.2022.110193},
  url = {https://www.sciencedirect.com/science/article/pii/S0950705122012898},
  author = {Zhan, Qiugang and Liu, Guisong and Xie, Xiurui and Zhang, Malu and Sun, Guolin},
  abstract = {Spiking neural networks (SNNs) have gained a lot of attention and achievements recently because of their low-power advantages on neuromorphic hardware. However, training deep SNNs still requires a large number of labeled data which are expensive to obtain. To address this issue, we propose an effective Bio-inspired Active Learning (BAL) method in this paper to reduce the training cost of SNN models. Specifically, bio-inspired behavior patterns of spiking neurons are defined to represent the internal states of SNN models for active learning. Then, an active learning sample selection strategy is proposed by leveraging the empirical and generalization pattern divergence in SNNs. By labeling selected samples and adding them to training, behavioral patterns can be optimized to improve the performance of neural networks. Comprehensive experiments are conducted on the CIFAR-10, SVHN, and Fashion-MNIST datasets with various sample proportions. The experimental results demonstrate that the proposed BAL achieves state-of-the-arts performance in SNNs compared with the existing active learning methods.}
}22:T4a3,A large quantity of labeled data is required to train high-performance deep spiking neural networks (SNNs), but obtaining labeled data is expensive. Active learning is proposed to reduce the quantity of labeled data required by deep learning models. However, conventional active learning methods in SNNs are not as effective as that in conventional artificial neural networks (ANNs) because of the difference in feature representation and information transmission. To address this issue, we propose an effective active learning method for a deep SNN model in this article. Specifically, a loss prediction module ActiveLossNet is proposed to extract features and select valuable samples for deep SNNs. Then, we derive the corresponding active learning algorithm for deep SNN models. Comprehensive experiments are conducted on CIFAR-10, MNIST, Fashion-MNIST, and SVHN on different SNN frameworks, including seven-layer CIFARNet and 20-layer ResNet-18. The comparison results demonstrate that the proposed active learning algorithm outperforms random selection and conventional ANN active learning methods. In addition, our method converges faster than conventional active learning methods.23:T644,@article{xie2023effective,
  title = {Effective active learning method for spiking neural networks},
  author = {Xie, Xiurui and Yu, Bei and Liu, Guisong and Zhan, Qiugang and Tang, Huajin},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {35},
  number = {9},
  pages = {12373--12382},
  year = {2023},
  publisher = {IEEE},
  abstract = {A large quantity of labeled data is required to train high-performance deep spiking neural networks (SNNs), but obtaining labeled data is expensive. Active learning is proposed to reduce the quantity of labeled data required by deep learning models. However, conventional active learning methods in SNNs are not as effective as that in conventional artificial neural networks (ANNs) because of the difference in feature representation and information transmission. To address this issue, we propose an effective active learning method for a deep SNN model in this article. Specifically, a loss prediction module ActiveLossNet is proposed to extract features and select valuable samples for deep SNNs. Then, we derive the corresponding active learning algorithm for deep SNN models. Comprehensive experiments are conducted on CIFAR-10, MNIST, Fashion-MNIST, and SVHN on different SNN frameworks, including seven-layer CIFARNet and 20-layer ResNet-18. The comparison results demonstrate that the proposed active learning algorithm outperforms random selection and conventional ANN active learning methods. In addition, our method converges faster than conventional active learning methods.},
  doi = {10.1109/TNNLS.2023.3257333}
}24:T42f,As the third generation of neural networks, spiking neural networks (SNNs) have gained much attention recently because of their high energy efficiency on neuromorphic hardware. However, training deep SNNs requires many labeled data that are expensive to obtain in real-world applications, as traditional artificial neural networks (ANNs). In order to address this issue, transfer learning has been proposed and widely used in traditional ANNs, but it has limited use in SNNs. In this article, we propose an effective transfer learning framework for deep SNNs based on the domain in-variance representation. Specifically, we analyze the rationality of centered kernel alignment (CKA) as a domain distance measurement relative to maximum mean discrepancy (MMD) in deep SNNs. In addition, we study the feature transferability across different layers by testing on the Office-31, Office-Caltech-10, and PACS datasets. The experimental results demonstrate the transferability of SNNs and show the effectiveness of the proposed transfer learning framework by using CKA in SNNs.25:T5bf,@ARTICLE{zhan2022effective,
  author = {Zhan, Qiugang and Liu, Guisong and Xie, Xiurui and Sun, Guolin and Tang, Huajin},
  journal = {IEEE Transactions on Cybernetics},
  title = {Effective Transfer Learning Algorithm in Spiking Neural Networks},
  year = {2022},
  volume = {52},
  number = {12},
  pages = {13323-13335},
  publisher = {IEEE},
  abstract = {As the third generation of neural networks, spiking neural networks (SNNs) have gained much attention recently because of their high energy efficiency on neuromorphic hardware. However, training deep SNNs requires many labeled data that are expensive to obtain in real-world applications, as traditional artificial neural networks (ANNs). In order to address this issue, transfer learning has been proposed and widely used in traditional ANNs, but it has limited use in SNNs. In this article, we propose an effective transfer learning framework for deep SNNs based on the domain in-variance representation. Specifically, we analyze the rationality of centered kernel alignment (CKA) as a domain distance measurement relative to maximum mean discrepancy (MMD) in deep SNNs. In addition, we study the feature transferability across different layers by testing on the Office-31, Office-Caltech-10, and PACS datasets. The experimental results demonstrate the transferability of SNNs and show the effectiveness of the proposed transfer learning framework by using CKA in SNNs.},
  doi = {10.1109/TCYB.2021.3079097}
}7:["$","div",null,{"className":"max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-12","children":[["$","$L15",null,{"config":{"type":"publication","title":"Publications","description":"A collection of my research work.","source":"publications.bib"},"publications":[{"id":"zhan2025sfedca","title":"SFedCA: Credit Assignment-Based Active Client Selection Strategy for Spiking Federated Learning","authors":[{"name":"Qiugang Zhan","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Jinbo Cao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xiurui Xie","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Huajin Tang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Malu Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Shantian Yang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Guisong Liu","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false}],"year":2025,"type":"journal","status":"published","tags":["Neurons;Data models;Training;Firing;Servers;Convergence;Membrane potentials;Biological system modeling;Federated learning;Adaptation models;Data heterogeneity;federated learning (FL);selection strategy;spiking neural network (SNN)"],"keywords":"$7:props:children:0:props:publications:0:tags","researchArea":"neural-networks","journal":"IEEE Transactions on Neural Networks and Learning Systems","conference":"","volume":"","issue":"","pages":"1-13","doi":"10.1109/TNNLS.2025.3639578","code":"","abstract":"$16","description":"We propose an active client selection method for spiking federated learning. This method assign credit for clients according to the firing intensity changes.","selected":true,"preview":"SFedCA.png","bibtex":"$17"},{"id":"xing2025flexible","title":"Flexible sharpness-aware personalized federated learning","authors":[{"name":"Xinda Xing","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Qiugang Zhan(共一)","isHighlighted":true,"isCorresponding":false,"isCoAuthor":true},{"name":"Xiurui Xie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yuning Yang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Qiang Wang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Guisong Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"conference","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:1:tags","researchArea":"machine-learning","journal":"","conference":"Proceedings of the AAAI Conference on Artificial Intelligence","volume":"39","issue":"20","pages":"21707--21715","doi":"10.1609/aaai.v39i20.35475","code":"","abstract":"$18","description":"We propose a simple and general PFL method, Federated learning with Flexible Sharpness-Aware Minimization (FedFSA). It emphasize the importance of applying a larger perturbation to critical layers of the local model when using the Sharpness-Aware Minimization (SAM) optimizer.","selected":false,"preview":"FedFSA.png","bibtex":"$19"},{"id":"ZHAN2024111220","title":"A two-stage spiking meta-learning method for few-shot classification","authors":[{"name":"Qiugang Zhan","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Bingchao Wang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Anning Jiang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xiurui Xie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Malu Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Guisong Liu","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false}],"year":2024,"type":"journal","status":"published","tags":["Spiking neural networks","Meta-learning","Centered Kernel alignment","Few-shot classification"],"keywords":"$7:props:children:0:props:publications:2:tags","researchArea":"neural-networks","journal":"Knowledge-Based Systems","conference":"","volume":"284","pages":"111220","doi":"https://doi.org/10.1016/j.knosys.2023.111220","url":"https://www.sciencedirect.com/science/article/pii/S095070512300970X","code":"","abstract":"$1a","description":"we explore a two-stage metric-based SNN meta-learning framework. During pre-training, a CESM model is trained to extract image features. In the meta-training stage, the MESM model employs the CKA method to measure the similarity between these learned features for meta-learning.","selected":true,"preview":"meta learning.png","bibtex":"$1b"},{"id":"zhan2024spiking","title":"Spiking Transfer Learning From RGB Image to Neuromorphic Event Stream","authors":[{"name":"Qiugang Zhan","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Guisong Liu","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Xiurui Xie","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Ran Tao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Malu Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Huajin Tang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2024,"type":"journal","status":"published","tags":["Transfer learning;Cameras;Streaming media;Data models;Training;Loss measurement;Brightness;Spiking neural networks;transfer learning;event camera"],"keywords":"$7:props:children:0:props:publications:3:tags","researchArea":"neural-networks","journal":"IEEE Transactions on Image Processing","conference":"","volume":"33","issue":"","pages":"4274-4287","doi":"10.1109/TIP.2024.3430043","code":"","abstract":"$1c","description":"To take advantage of both the rich knowledge in labeled RGB images and the features of the event camera, we propose a transfer learning method from the RGB to the event domain.","selected":true,"preview":"R2ETL.png","bibtex":"$1d"},{"id":"xie2024federated","title":"Federated learning for spiking neural networks by hint-layer knowledge distillation","authors":[{"name":"Xiurui Xie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jingxuan Feng","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Guisong Liu","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Qiugang Zhan","isHighlighted":true,"isCorresponding":true,"isCoAuthor":false},{"name":"Zhetong Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Malu Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2024,"type":"journal","status":"published","tags":["Spiking neural networks","Federated learning","Knowledge distillation","Communication costs","Hint-layer knowledge learning"],"keywords":"$7:props:children:0:props:publications:4:tags","researchArea":"neural-networks","journal":"Applied Soft Computing","conference":"","volume":"163","pages":"111901","doi":"https://doi.org/10.1016/j.asoc.2024.111901","url":"https://www.sciencedirect.com/science/article/pii/S1568494624006756","code":"","abstract":"$1e","description":"We propose a Hint-layer Distillation-based Spiking Federated Learning (HDSFL) framework that reduces the communication cost by transferring knowledge and losslessly compressing the spiking tensor.","selected":false,"preview":"HDSFL.png","bibtex":"$1f"},{"id":"ZHAN2023110193","title":"Bio-inspired Active Learning method in spiking neural network","authors":[{"name":"Qiugang Zhan","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Guisong Liu","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Xiurui Xie","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Malu Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Guolin Sun","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2023,"type":"journal","status":"published","tags":["Spiking neural networks","Active learning","Bio-inspired behavior patterns"],"keywords":"$7:props:children:0:props:publications:5:tags","researchArea":"neural-networks","journal":"Knowledge-Based Systems","conference":"","volume":"261","pages":"110193","doi":"https://doi.org/10.1016/j.knosys.2022.110193","url":"https://www.sciencedirect.com/science/article/pii/S0950705122012898","code":"","abstract":"$20","description":"We propose an effective Bio-inspired Active Learning (BAL) method to reduce the training cost of SNN models. Bio-inspired behavior patterns of spiking neurons are defined to represent the internal states of SNN models for active learning.","selected":true,"preview":"active learning.png","bibtex":"$21"},{"id":"xie2023effective","title":"Effective active learning method for spiking neural networks","authors":[{"name":"Xiurui Xie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Bei Yu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Guisong Liu","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Qiugang Zhan","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Huajin Tang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2023,"type":"journal","status":"published","tags":["Biological system modeling;Neurons;Learning systems;Predictive models;Training;Task analysis;Integrated circuit modeling;Active learning method;deep learning;feature representation;spiking neural network (SNN)"],"keywords":"$7:props:children:0:props:publications:6:tags","researchArea":"neural-networks","journal":"IEEE Transactions on Neural Networks and Learning Systems","conference":"","volume":"35","issue":"9","pages":"12373--12382","doi":"10.1109/TNNLS.2023.3257333","code":"","abstract":"$22","description":"We propose an effective active learning method with a loss prediction module for a deep SNN model.","selected":false,"preview":"TNNLS SAL.png","bibtex":"$23"},{"id":"zhan2022effective","title":"Effective Transfer Learning Algorithm in Spiking Neural Networks","authors":[{"name":"Qiugang Zhan","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Guisong Liu","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Xiurui Xie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Guolin Sun","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Huajin Tang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2022,"type":"journal","status":"published","tags":["Transfer learning;Deep learning;Neurons;Kernel;Feature extraction;Artificial neural networks;Membrane potentials;Centered kernel alignment;deep learning;spiking neural network (SNN);transfer learning"],"keywords":"$7:props:children:0:props:publications:7:tags","researchArea":"neural-networks","journal":"IEEE Transactions on Cybernetics","conference":"","volume":"52","issue":"12","pages":"13323-13335","doi":"10.1109/TCYB.2021.3079097","code":"https://github.com/QgZhan/SNNTL","abstract":"$24","description":"We propose the first transfer learning framework in SNN, and the rationality of centered kernel alignment (CKA) as a domain distance measurement relative to maximum mean discrepancy (MMD) in deep SNNs.","selected":true,"preview":"SNNTL.png","bibtex":"$25"}]}],false,false]}]
c:null
10:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
b:null
14:{"metadata":[["$","title","0",{"children":"Publications | Qiugang Zhan"}],["$","meta","1",{"name":"description","content":"A collection of my research work."}],["$","meta","2",{"name":"author","content":"Qiugang Zhan"}],["$","meta","3",{"name":"keywords","content":"Qiugang Zhan,PhD,Research,Southwestern University of Finance and Economics (SWUFE)"}],["$","meta","4",{"name":"creator","content":"Qiugang Zhan"}],["$","meta","5",{"name":"publisher","content":"Qiugang Zhan"}],["$","meta","6",{"property":"og:title","content":"Qiugang Zhan"}],["$","meta","7",{"property":"og:description","content":"Lecturer at the Southwestern University of Finance and Economics (SWUFE)."}],["$","meta","8",{"property":"og:site_name","content":"Qiugang Zhan's Academic Website"}],["$","meta","9",{"property":"og:locale","content":"en_US"}],["$","meta","10",{"property":"og:type","content":"website"}],["$","meta","11",{"name":"twitter:card","content":"summary"}],["$","meta","12",{"name":"twitter:title","content":"Qiugang Zhan"}],["$","meta","13",{"name":"twitter:description","content":"Lecturer at the Southwestern University of Finance and Economics (SWUFE)."}],["$","link","14",{"rel":"icon","href":"/favicon.svg"}]],"error":null,"digest":"$undefined"}
e:{"metadata":"$14:metadata","error":null,"digest":"$undefined"}
